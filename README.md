# ğŸ¤– Ollama Model Suite

<div align="center">

![Python](https://img.shields.io/badge/python-3.11+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-0.115+-00a393.svg)
![LangChain](https://img.shields.io/badge/LangChain-0.3.0+-1C3C3C.svg)
![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=flat&logo=docker&logoColor=white)
![Ollama](https://img.shields.io/badge/Ollama-000000?style=flat&logo=ollama&logoColor=white)
![License](https://img.shields.io/badge/license-MIT-green.svg)

Una suite completa para ejecutar y gestionar mÃºltiples modelos de lenguaje usando Ollama con un BFF (Backend for Frontend) construido con FastAPI y LangChain.

[CaracterÃ­sticas](#-caracterÃ­sticas) â€¢
[InstalaciÃ³n](#-instalaciÃ³n) â€¢
[Uso](#-uso) â€¢
[API](#-documentaciÃ³n-de-la-api) â€¢
[ConfiguraciÃ³n](#-configuraciÃ³n)

</div>

---

## ğŸŒŸ CaracterÃ­sticas

### ğŸš€ **GestiÃ³n Multi-Modelo**
- âœ… Soporte para mÃºltiples modelos LLM simultÃ¡neamente
- ğŸ”„ Descarga automÃ¡tica de modelos (pull)
- ğŸ“Š Lista dinÃ¡mica de modelos disponibles
- ğŸ”’ Control de acceso por modelo (whitelist opcional)

### ğŸ”§ **API REST Completa**
- ğŸ’¬ **Chat Completions** - Conversaciones con IA
- ğŸ§® **Embeddings** - GeneraciÃ³n de vectores para texto
- ğŸ“¥ **Model Management** - Descarga y gestiÃ³n de modelos
- ğŸ” **Health Checks** - Monitoreo del sistema

### ğŸ›¡ï¸ **CaracterÃ­sticas Avanzadas**
- ğŸŒ **CORS configurable** para desarrollo frontend
- âš¡ **Async/Await** para alto rendimiento
- ğŸ“ **ValidaciÃ³n con Pydantic** para datos seguros
- ğŸ³ **ContainerizaciÃ³n completa** con Docker
- ğŸ”§ **Variables de entorno** para configuraciÃ³n flexible

---

## ğŸ—ï¸ Arquitectura

```mermaid
graph TB
    subgraph "Cliente"
        FE[Frontend/Cliente]
    end
    
    subgraph "Docker Compose"
        subgraph "BFF Container"
            FA[FastAPI + LangChain]
        end
        
        subgraph "Ollama Container"
            OL[Ollama Server]
            M1[Modelo llama3.1]
            M2[Modelo deepseek-r1]
            M3[Modelo gpt-oss]
        end
        
        subgraph "Init Container"
            INIT[Descarga automÃ¡tica<br/>de modelos]
        end
    end
    
    FE -->|HTTP/REST| FA
    FA -->|LangChain| OL
    OL --> M1
    OL --> M2
    OL --> M3
    INIT -->|Pull Models| OL
```

---

## ğŸ“¦ InstalaciÃ³n

### Prerrequisitos

- ğŸ³ [Docker](https://www.docker.com/get-started) y Docker Compose
- ğŸ“¦ Git para clonar el repositorio

### Inicio RÃ¡pido

1. **Clona el repositorio**
   ```bash
   git clone https://github.com/tu-usuario/ollama-model-suite.git
   cd ollama-model-suite
   ```

2. **Inicia todos los servicios**
   ```bash
   docker-compose up -d
   ```

3. **Verifica que todo funcione**
   ```bash
   curl http://localhost:9900/health
   ```

4. **Â¡Listo!** ğŸ‰
   - ğŸ¤– **Ollama**: `http://localhost:11434`
   - ğŸ”Œ **API BFF**: `http://localhost:9900`
   - ğŸ“š **DocumentaciÃ³n**: `http://localhost:9900/docs`

---

## ğŸš€ Uso

### ğŸ’¬ Chat con IA

```bash
curl -X POST "http://localhost:9900/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {"role": "user", "content": "Â¿QuÃ© es la inteligencia artificial?"}
    ],
    "model": "llama3.1:latest",
    "temperature": 0.7
  }'
```

### ğŸ§® Generar Embeddings

```bash
curl -X POST "http://localhost:9900/embeddings" \
  -H "Content-Type: application/json" \
  -d '{
    "input": ["Hola mundo", "Â¿CÃ³mo estÃ¡s?"],
    "model": "llama3.1:latest"
  }'
```

### ğŸ“¥ Descargar Nuevos Modelos

```bash
curl -X POST "http://localhost:9900/pull" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "mistral:latest"
  }'
```

### ğŸ“Š Listar Modelos Disponibles

```bash
curl http://localhost:9900/models
```

---

## ğŸ“‹ DocumentaciÃ³n de la API

### ğŸ”— Endpoints Principales

| MÃ©todo | Endpoint | DescripciÃ³n |
|--------|----------|-------------|
| `GET` | `/health` | Estado del sistema |
| `GET` | `/models` | Lista de modelos disponibles |
| `POST` | `/chat` | ConversaciÃ³n con IA |
| `POST` | `/embeddings` | Generar vectores de texto |
| `POST` | `/pull` | Descargar modelo |
| `GET` | `/docs` | DocumentaciÃ³n interactiva Swagger |

### ğŸ“– DocumentaciÃ³n Interactiva

Una vez que tengas el sistema ejecutÃ¡ndose, visita:
- ğŸ“š **Swagger UI**: `http://localhost:9900/docs`
- ğŸ“„ **ReDoc**: `http://localhost:9900/redoc`

---

## âš™ï¸ ConfiguraciÃ³n

### ğŸŒ Variables de Entorno

| Variable | Por Defecto | DescripciÃ³n |
|----------|-------------|-------------|
| `OLLAMA_BASE` | `http://ollama:11434` | URL base de Ollama |
| `DEFAULT_MODEL` | `llama3.1:latest` | Modelo por defecto |
| `ALLOWED_MODELS` | `gpt-oss:latest,deepseek-r1:latest,llama3.1:latest` | Modelos permitidos |
| `CORS_ORIGINS` | `*` | OrÃ­genes CORS permitidos |
| `OLLAMA_MODELS` | `gpt-oss:latest,deepseek-r1:latest,llama3.1:latest` | Modelos a descargar automÃ¡ticamente |

### ğŸ”§ PersonalizaciÃ³n

#### Cambiar Modelos por Defecto

Edita el archivo `docker-compose.yml`:

```yaml
environment:
  - OLLAMA_MODELS=llama3.1:latest,mistral:latest,codellama:latest
  - DEFAULT_MODEL=mistral:latest
  - ALLOWED_MODELS=llama3.1:latest,mistral:latest,codellama:latest
```

#### Configurar CORS para ProducciÃ³n

```yaml
environment:
  - CORS_ORIGINS=https://miapp.com,https://www.miapp.com
```

---

## ğŸ—ï¸ Estructura del Proyecto

```
ollama-model-suite/
â”œâ”€â”€ ğŸ“„ docker-compose.yml      # OrquestaciÃ³n de servicios
â”œâ”€â”€ ğŸ“ bff/                    # Backend for Frontend
â”‚   â”œâ”€â”€ ğŸ app.py             # AplicaciÃ³n FastAPI principal
â”‚   â”œâ”€â”€ ğŸ“‹ requirements.txt    # Dependencias Python
â”‚   â”œâ”€â”€ ğŸ³ Dockerfile         # Imagen del BFF
â”‚   â””â”€â”€ ğŸ“ schemas.py         # Esquemas Pydantic
â”œâ”€â”€ ğŸ“„ .gitignore             # Archivos ignorados por Git
â””â”€â”€ ğŸ“– README.md              # Este archivo
```

---

## ğŸ› ï¸ Stack TecnolÃ³gico

### Backend
- ğŸ **Python 3.11+** - Lenguaje principal
- âš¡ **FastAPI** - Framework web moderno y rÃ¡pido
- ğŸ”— **LangChain** - Framework para aplicaciones con LLM
- ğŸ“Š **Pydantic** - ValidaciÃ³n de datos
- ğŸŒ **httpx** - Cliente HTTP asÃ­ncrono
- ğŸ¦„ **Uvicorn** - Servidor ASGI

### Infraestructura
- ğŸ³ **Docker & Docker Compose** - ContainerizaciÃ³n
- ğŸ¤– **Ollama** - Motor de modelos LLM
- ğŸ”„ **CORS** - Soporte para aplicaciones web

### Modelos Incluidos
- ğŸ¦™ **Llama 3.1** - Modelo conversacional avanzado
- ğŸ§  **DeepSeek R1** - Modelo de razonamiento
- ğŸ’» **GPT-OSS** - Modelo de cÃ³digo abierto

---

## ğŸš¦ Comandos Ãštiles

### ğŸ”„ GestiÃ³n de Servicios

```bash
# Iniciar todos los servicios
docker-compose up -d

# Ver logs en tiempo real
docker-compose logs -f

# Parar todos los servicios
docker-compose down

# Reconstruir imÃ¡genes
docker-compose build --no-cache

# Reiniciar solo el BFF
docker-compose restart bff
```

### ğŸ” Debugging

```bash
# Ver logs del BFF
docker-compose logs bff

# Ver logs de Ollama
docker-compose logs ollama

# Ejecutar comandos dentro del contenedor BFF
docker-compose exec bff bash

# Ver estado de los contenedores
docker-compose ps
```

---

## ğŸ¤ Contribuir

Â¡Las contribuciones son bienvenidas! ğŸ‰

1. ğŸ´ Fork el proyecto
2. ğŸŒ± Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. ğŸ’¾ Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. ğŸ“¤ Push a la rama (`git push origin feature/AmazingFeature`)
5. ğŸ”ƒ Abre un Pull Request

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver el archivo `LICENSE` para mÃ¡s detalles.

---