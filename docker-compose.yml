version: "3.8"

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_KEEP_ALIVE=24h
    volumes:
      - ollama:/root/.ollama

  ollama-init:
    image: curlimages/curl:8.10.1
    container_name: ollama-init
    depends_on:
      - ollama
    environment:
      - OLLAMA_MODELS=qwen3:8b
    entrypoint: >
      sh -lc '
        until curl -s http://ollama:11434/api/tags >/dev/null; do
          echo "Esperando a Ollama..."; sleep 2;
        done
        IFS=, 
        for M in $${OLLAMA_MODELS}; do
          M=$$(echo $$M | xargs)   # trim espacios
          [ -z "$$M" ] && continue
          echo ">> Pull $$M"
          curl -s -X POST http://ollama:11434/api/pull \
               -H "Content-Type: application/json" \
               -d "{\"name\":\"$$M\"}" \
          || echo "pull fall√≥ para $$M"
        done
        echo "Modelos listos."
      '


  bff:
    build: ./bff
    container_name: bff
    depends_on:
      - ollama
    environment:
      - OLLAMA_BASE=http://ollama:11434
      # Modelo por default si no lo mandan en el body
      - DEFAULT_MODEL=qwen3:8b
      # Opcional: lista blanca
      - ALLOWED_MODELS=qwen3:8b
      - CORS_ORIGINS=*
    ports:
      - "9900:9900"

  openwebui:
    image: ghcr.io/open-webui/open-webui:latest
    container_name: openwebui
    restart: unless-stopped
    depends_on:
      - ollama
      - bff
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://bff:9900
      - OLLAMA_API_BASE_URL=bff:9900
    volumes:
      - openwebui:/app/backend/data


volumes:
  ollama:
  openwebui:
