version: "3.8"

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_KEEP_ALIVE=24h
    volumes:
      - ollama:/root/.ollama

  ollama-init:
    image: curlimages/curl:8.10.1
    container_name: ollama-init
    depends_on:
      - ollama
    environment:
      - OLLAMA_MODELS=deepseek-r1:1.5b,
    entrypoint: >
      sh -lc '
        until curl -s http://ollama:11434/api/tags >/dev/null; do
          echo "Esperando a Ollama..."; sleep 2;
        done
        IFS=, 
        for M in $${OLLAMA_MODELS}; do
          M=$$(echo $$M | xargs)   # trim espacios
          [ -z "$$M" ] && continue
          echo ">> Pull $$M"
          curl -s -X POST http://ollama:11434/api/pull \
               -H "Content-Type: application/json" \
               -d "{\"name\":\"$$M\"}" \
          || echo "pull fall√≥ para $$M"
        done
        echo "Modelos listos."
      '


  bff:
    build: ./bff
    container_name: langchain-bff
    depends_on:
      - ollama
    environment:
      - OLLAMA_BASE=http://ollama:11434
      # Modelo por default si no lo mandan en el body
      - DEFAULT_MODEL=llama3.1:latest
      # Opcional: lista blanca
      - ALLOWED_MODELS=gpt-oss:latest,deepseek-r1:latest,llama3.1:latest,deepseek-r1:1.5b
      - CORS_ORIGINS=*
    ports:
      - "9900:9900"

volumes:
  ollama:
